import feedparser
from datetime import datetime, date, timedelta
import pandas as pd
import time
import json
import re
from openai import OpenAI
from tqdm import tqdm
from xml.etree.ElementTree import Element, SubElement, ElementTree
from email.utils import format_datetime
from pathlib import Path
import subprocess

# ========== é…ç½®åŒºåŸŸ ==========

# RSS æºåœ°å€
RSS_FEEDS = {
    "ChainFeeds": "https://www.chainfeeds.xyz/rss",
    "PANews_æ¨è": "https://www.panewslab.com/zh/rss/foryou.xml",
    "BlockBeats": "https://api.theblockbeats.news/v2/rss/all",
    "PANews_å¿«è®¯": "https://www.panewslab.com/zh/rss/newsflash.xml"
}

# DeepSeek API Key
API_KEY = "sk-23abfd1963d4433eaa8a5b3b0b2ba22a"  # <-- æ›¿æ¢ä¸ºä½ è‡ªå·±çš„ API Key

HISTORY_FILE = r"/Users/kay/Desktop/rss/rss_history.csv"
XML_OUTPUT   = r"/Users/kay/Desktop/rss/filtered_feed.xml"
HISTORY_DAYS = 30

# ========== å·¥å…·å‡½æ•° ==========
def git_push_rss_file(file_path, commit_message="update RSS XML"):
    try:
        subprocess.run(["git", "add", file_path], check=True)
        subprocess.run(["git", "commit", "-m", commit_message], check=True)
        subprocess.run(["git", "push"], check=True)
        print("ğŸš€ å·²è‡ªåŠ¨æ¨é€ RSS åˆ° GitHub")
    except subprocess.CalledProcessError as e:
        print(f"âŒ Git æ“ä½œå¤±è´¥ï¼š{e}")

def is_yesterday(struct_time_obj):
    if not struct_time_obj:
        return False
    published_date = datetime(*struct_time_obj[:6]).date()
    return published_date == (date.today() - timedelta(days=1))

def parse_rss_feed(name, url):
    feed = feedparser.parse(url)
    entries = []
    for entry in feed.entries:
        published_parsed = entry.get("published_parsed") or entry.get("updated_parsed")
        if is_yesterday(published_parsed):
            published_str = time.strftime("%Y-%m-%d", published_parsed)
            entries.append({
                "source": name,
                "title": entry.title,
                "link": entry.link,
                "summary": entry.summary,
                "time": published_str
            })
    return entries

def fetch_all_rss():
    all_entries = []
    for name, url in RSS_FEEDS.items():
        try:
            entries = parse_rss_feed(name, url)
            all_entries.extend(entries)
        except Exception as e:
            print(f"âŒ Failed to fetch {name}: {e}")
    return pd.DataFrame(all_entries)

def filter_by_keywords(df):
    black_keywords = [
        "æ¯”ç‰¹å¸ä»·æ ¼", "ä»¥å¤ªåŠä»·æ ¼", "è¡Œæƒ…", "è·Œç ´", "çˆ†ä»“", "Binance Alpha", "å¸å®‰ Alpha", "ä¸€å‘¨é¢„å‘Š"
    ]

    def is_useful(entry):
        text = f"{entry['title']} {entry['summary']}"
        return not any(bad in text for bad in black_keywords)
    return df[df.apply(is_useful, axis=1)]

# ========== DeepSeek åˆ†ç±»é€»è¾‘ ==========

def build_batch_prompt(news_batch):
    prompt = """
ä½ æ˜¯ä¸€åä¸“ä¸šçš„åŒºå—é“¾ç ”ç©¶å‘˜ã€‚ä¸‹é¢æ˜¯å¤šæ¡åŒºå—é“¾ç›¸å…³æ–°é—»ï¼Œè¯·ä½ é€æ¡åˆ¤æ–­ï¼š

å¯¹äºæ¯æ¡æ–°é—»ï¼Œè¯·åˆ¤æ–­ä»¥ä¸‹ä¸‰é¡¹å†…å®¹ï¼Œå¹¶è¿”å› JSON æ•°ç»„ï¼š
1. is_valuableï¼ˆæ˜¯å¦æœ‰ä»·å€¼ï¼‰ï¼šTrue æˆ– False
2. categoryï¼ˆè‹¥æœ‰ä»·å€¼ï¼Œè¯·åœ¨ä»¥ä¸‹å››ä¸ªç±»åˆ«ä¸­é€‰æ‹©ä¸€ä¸ªï¼‰ï¼š
   - äº¤äº’æœºä¼š
   - æŠ•èµ„å‚è€ƒ
   - é£é™©é¢„è­¦
   - è¡Œä¸šè¶‹åŠ¿ / çƒ­ç‚¹å™äº‹
3. suggest_actionï¼ˆè‹¥æœ‰ä»·å€¼ï¼Œä½ å»ºè®®çš„ä¸‹ä¸€æ­¥è¡ŒåŠ¨ï¼‰

è¯·ä¸¥æ ¼è¿”å›å¦‚ä¸‹æ ¼å¼çš„ JSON æ•°ç»„ï¼Œæ¯æ¡ç»“æœæŒ‰åºå¯¹åº”ï¼š
[
  {
    "is_valuable": true,
    "category": "...",
    "suggest_action": "..."
  },
  ...
]

ä»¥ä¸‹æ˜¯æ–°é—»åˆ—è¡¨ï¼š
"""
    for idx, row in enumerate(news_batch, 1):
        prompt += f"\n[{idx}] æ ‡é¢˜ï¼š{row['title']}\nå†…å®¹ï¼š{row['summary']}\n"

    return prompt

def classify_batch(entries, batch_size=10):
    all_results = []

    for i in tqdm(range(0, len(entries), batch_size)):
        batch = entries[i:i+batch_size]
        prompt = build_batch_prompt(batch)

        try:
            response = client.chat.completions.create(
                model="deepseek-chat",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.2
            )
            raw_text = response.choices[0].message.content

            # æå– JSONï¼ˆå¯åŠ å…¥é”™è¯¯å®¹é”™å¤„ç†ï¼‰
            match = re.search(r"\[.*\]", raw_text, re.DOTALL)
            json_text = match.group(0) if match else raw_text
            result_batch = json.loads(json_text)

            if len(result_batch) != len(batch):
                raise ValueError("è¿”å›ç»“æœæ•°é‡ä¸è¯·æ±‚æ•°é‡ä¸ä¸€è‡´")

            all_results.extend(result_batch)

        except Exception as e:
            print(f"âŒ æ‰¹æ¬¡ {i} å‡ºé”™ï¼š{e}")
            for _ in batch:
                all_results.append({
                    "is_valuable": False,
                    "category": None,
                    "suggest_action": f"Error: {e}"
                })

        time.sleep(2)  # æ§åˆ¶é¢‘ç‡

    return all_results

def generate_rss_xml(df, outfile):
    rss = Element("rss", version="2.0")
    channel = SubElement(rss, "channel")

    SubElement(channel, "title").text       = "DeepSeek Alpha Feed"
    SubElement(channel, "link").text        = "https://zzzzzioi.github.io/rss_alpha/filtered_feed.xml"
    SubElement(channel, "description").text = "è¿‘ 30 å¤©é«˜ä»·å€¼åŒºå—é“¾èµ„è®¯"
    SubElement(channel, "lastBuildDate").text = format_datetime(datetime.now())

    for _, row in df.iterrows():
        item = SubElement(channel, "item")
        SubElement(item, "title").text       = row["title"]
        SubElement(item, "link").text        = row["link"]
        SubElement(item, "description").text = f"ç±»åˆ«ï¼š{row['category']} | å»ºè®®ï¼š{row['suggest_action']}"
        SubElement(item, "summary").text     = row['summary']
        SubElement(item, "pubDate").text     = format_datetime(pd.to_datetime(row["time"]))
        SubElement(item, "source").text      = row["source"]

    ElementTree(rss).write(outfile, encoding="utf-8", xml_declaration=True)
    print(f"âœ… RSS XML å·²æ›´æ–°ï¼š{outfile}")
# ========== ä¸»æ‰§è¡Œå…¥å£ ==========

if __name__ == "__main__":
    print("ğŸ” å¼€å§‹æŠ“å–æ˜¨æ—¥ RSS...")
    df_rss = fetch_all_rss()

    print("ğŸ§¹ å¼€å§‹å…³é”®è¯åˆç­›...")
    df_filtered = filter_by_keywords(df_rss)
    print(f"âœ… åˆç­›åä¿ç•™ {len(df_filtered)} æ¡")

    print("ğŸ¤– æ­£åœ¨è°ƒç”¨ DeepSeek è¿›ä¸€æ­¥ç­›é€‰åˆ†ç±»...")

    client = OpenAI(api_key=API_KEY, base_url="https://api.deepseek.com")

    results = classify_batch(df_filtered.to_dict("records"), batch_size=10)

    df_filtered.loc[:, "is_valuable"] = [r["is_valuable"] for r in results]
    df_filtered.loc[:, "category"] = [r["category"] for r in results]
    df_filtered.loc[:, "suggest_action"] = [r["suggest_action"] for r in results]

    valuable_df = df_filtered[df_filtered["is_valuable"] == True].copy()
    valuable_df.loc[:, "time"] = pd.to_datetime(valuable_df["time"])

    if not valuable_df.empty:
        if Path(HISTORY_FILE).exists():
            df_history = pd.read_csv(HISTORY_FILE, parse_dates=["time"])
        else:
            df_history = pd.DataFrame(columns=valuable_df.columns)

        df_all = pd.concat([df_history, valuable_df], ignore_index=True)
        df_all = df_all.drop_duplicates(subset=["title", "link"])

        cutoff = datetime.now() - timedelta(days=HISTORY_DAYS)
        df_all = df_all[df_all["time"] >= cutoff]

        df_all.to_csv(HISTORY_FILE, index=False)
        generate_rss_xml(df_all, XML_OUTPUT)       # æ›´æ–° filtered_feed.xml
    else:
        print("â• æœ¬æ¬¡æ²¡æœ‰æ–°çš„æœ‰ä»·å€¼ä¿¡æ¯ï¼Œå†å² RSS ä¿æŒä¸å˜")
